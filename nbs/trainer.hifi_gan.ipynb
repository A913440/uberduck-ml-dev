{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43958454",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer.hifi_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "\n",
    "from uberduck_ml_dev.data_loader import AudioMelDataset\n",
    "from uberduck_ml_dev.models.hifi_gan import (\n",
    "    Generator,\n",
    "    MultiPeriodDiscriminator,\n",
    "    MultiScaleDiscriminator,\n",
    ")\n",
    "from uberduck_ml_dev.trainer.base import TTSTrainer\n",
    "\n",
    "\n",
    "class HifiGANTrainer(TTSTrainer):\n",
    "    @property\n",
    "    def training_dataset_args(self):\n",
    "        return [\n",
    "            self.training_audiopaths_and_text,\n",
    "            self.segment_size,\n",
    "            self.n_mel_channels,\n",
    "            self.sample_rate,\n",
    "            self.mel_fmin,\n",
    "            self.mel_fmax,\n",
    "            self.mel_fmax_for_loss,\n",
    "            self.filter_length,\n",
    "            self.hop_length,\n",
    "            self.win_length,\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def val_dataset_args(self):\n",
    "        val_args = [a for a in self.training_dataset_args]\n",
    "        val_args[0] = self.val_audiopaths_and_text\n",
    "        return val_args\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        generator = Generator(self.hparams)\n",
    "        mpd = MultiPeriodDiscriminator()\n",
    "        msd = MultiScaleDiscriminator()\n",
    "\n",
    "        optim_g = AdamW(\n",
    "            generator.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            betas=[self.adam_b1, self.adam_b2],\n",
    "        )\n",
    "        optim_d = AdamW(\n",
    "            itertools.chain(mpd.parameters(), msd.parameters()),\n",
    "            lr=self.learning_rate,\n",
    "            betas=[self.adam_b1, self.adam_b2],\n",
    "        )\n",
    "        scheduler_g = ExponentialLR(\n",
    "            optim_g, gamma=self.lr_decay, last_epoch=self.epochs\n",
    "        )\n",
    "        scheduler_d = ExponentialLR(\n",
    "            optim_d, gamma=self.lr_decay, last_epoch=self.epochs\n",
    "        )\n",
    "\n",
    "        generator.train()\n",
    "        mpd.train()\n",
    "        msd.train()\n",
    "        # main training loop\n",
    "        for epoch in range(start_epoch, self.epochs):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23756a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioMelDataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d23ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
